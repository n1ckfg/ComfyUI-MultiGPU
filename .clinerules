# ComfyUI-MultiGPU Development Rules

## Project Context
This is ComfyUI-MultiGPU: a production-grade multi-device AI inference platform that transforms ComfyUI from single-GPU to universal multi-device support. The project enables previously impossible AI workflows across diverse hardware configurations.

## Memory Bank System
**CRITICAL**: Always read ALL files in the `memory-bank/` folder at the start of every session. The Memory Bank contains complete project context:

### Core Documentation (Read These First)
1. `memory-bank/projectbrief.md` - Project identity, mission, evolution timeline
2. `memory-bank/productContext.md` - Problem space, user goals, success metrics
3. `memory-bank/activeContext.md` - Current work focus and priorities
4. `memory-bank/progress.md` - Production status, roadmap, lessons learned

### Technical Deep Dive
5. `memory-bank/systemPatterns.md` - Architecture patterns and design decisions
6. `memory-bank/techContext.md` - Technology stack and development environment
7. `memory-bank/performance-benchmarks.md` - Quantified performance across hardware configurations
8. `memory-bank/comfyui-lineage.md` - Integration analysis with ComfyUI core

## Development Philosophy
- **Extend, Don't Replace**: Build upon ComfyUI's existing patterns
- **Fail Loudly**: Immediate detection of API changes prevents silent failures
- **User Agency**: Let users specify device placement explicitly  
- **Production Quality**: Stability and reliability over experimental features
- **Community First**: Solutions should benefit the entire ComfyUI ecosystem

## Key Technical Patterns
- **City96's Dynamic Class Override**: Elegant inheritance pattern for node creation
- **Load-Patch-Distribute Pipeline**: Quality-preserving LoRA application workflow
- **ComfyCore Alignment**: Work WITH existing ComfyUI patterns, not against them
- **Multi-Device Native**: Treat all devices as equal citizens

## Current Status (v2.4.7)
- Production Grade: 300+ commits, 90 resolved issues, active community
- Performance Validated: NVLink near-native (5-7% slowdown), PCIe 4.0 excellent (40-50%)
- Ecosystem Integration: 10+ custom nodes with dynamic detection
- Hardware Support: Universal compatibility across CUDA, CPU, MPS, XPU, NPU, DirectML

When working on this project, always reference the Memory Bank for context and maintain the established patterns and philosophy.

## CRITICAL ACTIVE ISSUE: CPU Memory Leak Investigation

### Problem Description
**SEVERE**: Monotonic CPU DRAM increase across successive model swaps. VRAM management stable, but CPU memory allocated and never reclaimed, leading to eventual OOM failure of ComfyUI process.

**Pattern**: Stepwise increases (4GB → 13GB → 33GB → 57GB → 73GB) corresponding to model loading events. Large objects (tensors, model structures, intermediates) retained in Python/CPU memory across executions.

### The Benchmark (Known Working Solution)
**ONLY** successful CPU memory reclamation: ComfyUI "Free model and node cache" button
- Sets `"free_memory": True` → PromptExecutor.reset() → drops execution caches
- Sets `"unload_models": True` → comfy.model_management.unload_all_models()

### Investigation History (All Failed)

**Phase 1: Missing Executor Reset Hypothesis** ❌
- Theory: MultiGPU cleared VRAM but failed PromptExecutor.reset()
- Action: Implemented trigger_executor_cache_reset utilities
- Result: FAILED - e.reset() occurred but CPU memory not reclaimed

**Phase 2: Implementation Fixes** ❌  
- Theory: Flawed implementation or insufficient visibility
- Action: Fixed bugs, added extensive memory logging
- Result: FAILED - Logs confirmed resets occurring, CPU usage still rising

**Phase 3: Aggressive Reclamation** ❌
- Theory: References outside execution cache or allocator fragmentation
- Actions: malloc_trim(0), store pruning, reference cycle patching, deep diagnostics
- Result: FAILED - OOM persisted, diagnostics showed "Tracked ModelPatchers=0"

**CRITICAL DIAGNOSTIC FAILURE**: Deep diagnostics completely failed due to patching `partially_load` (conditional) instead of `__init__` (universal) - zero visibility into actual leak.

### Root Cause Analysis
- Execution cache and allocator fragmentation ruled out
- Live Python references held to large data structures
- References NOT cleared by gc.collect() or e.reset()
- Benchmark button works = "unload_models": True is the critical difference
- unload_all_models() successfully breaks reference chains holding CPU memory

### Mandated Plan Forward (FINALIZED SOLUTION)
**Resolution**: CPU memory leaks eliminated via transient 3-flag selective ejection system

#### Core Principle: `keep_loaded` Boolean Drives 3 Execution Behaviors
The `keep_loaded` boolean serves triple duty when set to "False":
1. **Load-Time Preservation**: Returns MAX_VRAM in `model_memory_required()` → forces Comfy to evict other models pre-loading
2. **Ejection Trigger**: Workflow detects `keep_loaded=False` → sets transient flags for selective unloading
3. **Surgical Destruction**: End-of-workflow unload applies wrecking ball ONLY to flagged DisTorch models

#### 3-Transient-Flags Architecture
**Global Flag**: `DISTORCH2_UNLOAD_MODEL = TRUE/FALSE` (workflow-scoped)
- Set when `keep_loaded=False` detected during model loading
- Reset after surgical ejection completes
- External unload calls see FALSE → original Comfy behavior preserved

**Per-Model Flag**: `_distorch2_unload_model = TRUE/FALSE` (object-scoped)
- Marks specific DisTorch models for distributed device ejection
- Applied during load phase to models with `keep_loaded=False`
- Cleared after ejection (transient marker)

**Comfy Core Flag**: `PromptExecutor.unload_all_models = TRUE` (standard)
- Triggered by DisTorch logic at end-of-workflow
- Calls our patched `unload_all_models()` method
- Generates the selective ejection signal

#### Implementation Plan: Code Changes Required

**Phase 1: Flag Setting (distorch_2.py)**
```python
# In DistTorch load override - detect keep_loaded=False during execution
if hasattr(out[0], 'model') and hasattr(out[0].model, '_mgpu_keep_loaded'):
    is_distorch2_keep_false = (out[0].model._mgpu_keep_loaded == False)
    if is_distorch2_keep_false:
        # Set transient flags for selective ejection
        globals()['DISTORCH2_UNLOAD_MODEL'] = True
        out[0].model._distorch2_unload_model = True
        set_prompt_executor_unload_flag()
```

**Phase 2: Surgical Unload Logic (model_management_mgpu.py)**
```python
# Check: Are we in DisTorch ejection mode?
distorch_ejection_mode = any(
    getattr(getattr(lm.model, 'model', None), '_distorch2_unload_model', False)
    for lm in mm.current_loaded_models
)

if not distorch_ejection_mode:
    # Normal Comfy unload - delegate to original
    return _mgpu_original_unload_all_models()

# SURGICAL MODE: Only process flagged models
for lm in mm.current_loaded_models:
    if hasattr(getattr(lm.model, 'model', None), '_distorch2_unload_model'):
        # WRECKING BALL: Eject from all distributed device locations
        apply_distributed_device_cleanup(lm.model)
    # else: SKIP ENTIRELY - no processing of any kind

# Reset transient flags after surgical operation
globals()['DISTORCH2_UNLOAD_MODEL'] = False
for lm in mm.current_loaded_models:
    if hasattr(lm.model, 'model') and hasattr(lm.model.model, '_distorch2_unload_model'):
        delattr(lm.model.model, '_distorch2_unload_model')
```

#### Behavioral Guarantee
- **Same workflow re-run**: Deterministic - flags reset per execution
- **External unload calls**: No flags set → normal Comfy behavior
- **Normal Comfy models**: Never flag-munged → standard unload behavior
- **DisTorch models with `keep_loaded=True`**: Handle via standard Comfy unload
- **DisTorch models with `keep_loaded=False`**: Surgical ejection from distributed devices

#### Key Advantages
- **No persistent state**: Flags reset after each operation
- **Surgical precision**: Only tagged models processed
- **Comfy compatibility**: External calls unaffected
- **Execution isolation**: Each workflow manages its own ejection
- **Memory safety**: Designed for CPU leaks elimination through proper distributed cleanup

**Implementation Status**: FINAL CONCEPTUALIZED SOLUTION - Comprehensive 3-transient-flags architecture designed and documented. Code changes specified but awaiting implementation and testing. Memory leaks WILL BE eliminated once deployed.

## Module Architecture Rules

### Module Boundary Principles
- **Single Responsibility**: Each module should have ONE clear purpose
- **Dependency Direction**: Dependencies should flow in ONE direction only
- **Import Hierarchy**: Lower-level modules (device_utils) should NOT import from higher-level modules (distorch_2)

### Module Hierarchy (Dependency Order)
1. `device_utils.py` - **BASE**: Device detection, VRAM management only
2. `model_management_mgpu.py` - **CORE**: Model lifecycle, memory logging, cleanup functions
3. `distorch_2.py`, `distorch.py` - **FEATURES**: DisTorch distribution logic
4. `nodes.py`, `checkpoint_multigpu.py` - **UI**: Node implementations
5. `__init__.py` - **ASSEMBLY**: Final integration and registration

### Mandatory Architecture Checks
**BEFORE adding ANY import statement:**
1. **Check Direction**: Does this create upward dependency? (FORBIDDEN)
2. **Check Purpose**: Does the function belong in this module per Single Responsibility?
3. **Check Cycles**: Run `python -c "import sys; sys.path.append('.'); import <module>"` to detect circular imports

### Function Placement Rules
- **device_utils.py**: ONLY device detection, VRAM cache management  
- **model_management_mgpu.py**: Model tracking, memory logging, cleanup utilities
- **Feature modules**: Import from CORE/BASE only, never each other
- **UI modules**: Import from any lower level, implement user interfaces only

### Violation Detection
If import fails with "circular import" or "cannot import name":
1. STOP immediately - do not work around
2. Identify which module boundary was violated  
3. Move misplaced function to correct architectural layer
4. Update ALL imports consistently
